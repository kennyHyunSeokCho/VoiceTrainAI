import openai
import os
from pathlib import Path
import logging
from dotenv import load_dotenv
import ffmpeg
import tempfile
import shutil

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VoiceToText:
    """OpenAI API Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ í´ë˜ìŠ¤ (íŒŒì¼ ë¶„í•  ì§€ì›)"""
    
    def __init__(self, api_key=None, max_file_size_mb=20):
        """
        ì´ˆê¸°í™”
        
        Args:
            api_key (str): OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
            max_file_size_mb (int): ë¶„í•  ê¸°ì¤€ í¬ê¸° (MB) - OpenAI ì œí•œ 25MBë³´ë‹¤ ì‘ê²Œ ì„¤ì •
        """
        # API í‚¤ ì„¤ì •
        if api_key:
            self.api_key = api_key
        else:
            self.api_key = os.getenv('OPENAI_API_KEY')
        
        if not self.api_key:
            raise ValueError(
                "OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                "í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key ë§¤ê°œë³€ìˆ˜ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."
            )
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = openai.OpenAI(api_key=self.api_key)
        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024  # MB to bytes
        logger.info(f"OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ë¶„í•  ê¸°ì¤€: {max_file_size_mb}MB)")
    
    def split_audio_file(self, audio_path, target_size_mb=20):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì§€ì •ëœ í¬ê¸°ë¡œ ë¶„í• 
        
        Args:
            audio_path (str): ë¶„í• í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            target_size_mb (int): ëª©í‘œ ë¶„í•  í¬ê¸° (MB)
            
        Returns:
            list: ë¶„í• ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            probe = ffmpeg.probe(audio_path)
            duration = float(probe['streams'][0]['duration'])  # ì´ˆ ë‹¨ìœ„
            file_size = os.path.getsize(audio_path)
            
            # ë¶„í• ì´ í•„ìš”í•œì§€ í™•ì¸
            if file_size <= self.max_file_size_bytes:
                return [audio_path]  # ë¶„í•  ë¶ˆí•„ìš”
            
            # ë¶„í•  ê°œìˆ˜ ê³„ì‚°
            target_size_bytes = target_size_mb * 1024 * 1024
            num_parts = int(file_size / target_size_bytes) + 1
            segment_duration = duration / num_parts
            
            logger.info(f"íŒŒì¼ ë¶„í•  ì‹œì‘: {num_parts}ê°œ ì„¸ê·¸ë¨¼íŠ¸ (ê° {segment_duration:.1f}ì´ˆ)")
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_dir = tempfile.mkdtemp(prefix="vocal_split_")
            split_files = []
            
            base_name = Path(audio_path).stem
            
            for i in range(num_parts):
                start_time = i * segment_duration
                output_path = os.path.join(temp_dir, f"{base_name}_part{i+1}.wav")
                
                # FFmpegë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
                (
                    ffmpeg
                    .input(audio_path, ss=start_time, t=segment_duration)
                    .output(output_path, acodec='pcm_s16le', ar=16000)  # 16kHz, 16bit
                    .overwrite_output()
                    .run(quiet=True)
                )
                
                if os.path.exists(output_path):
                    split_files.append(output_path)
                    part_size = os.path.getsize(output_path) / (1024 * 1024)
                    logger.info(f"ë¶„í•  ì™„ë£Œ: part {i+1}/{num_parts} ({part_size:.1f}MB)")
            
            return split_files
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¶„í•  ì‹¤íŒ¨: {e}")
            raise
    
    def transcribe_audio(self, audio_path, language='ko'):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (OpenAI API ì‚¬ìš©, í•„ìš”ì‹œ ìë™ ë¶„í• )
        
        Args:
            audio_path (str): ë³€í™˜í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            language (str): ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸ê°’: 'ko' - í•œêµ­ì–´)
            
        Returns:
            dict: ë³€í™˜ ê²°ê³¼ (í…ìŠ¤íŠ¸, ì–¸ì–´, ê¸¸ì´ ì •ë³´ í¬í•¨)
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        
        logger.info(f"ìŒì„±ì¸ì‹ ì‹œì‘: {os.path.basename(audio_path)}")
        
        try:
            file_size = os.path.getsize(audio_path)
            
            # íŒŒì¼ í¬ê¸° í™•ì¸ ë° ë¶„í•  ì—¬ë¶€ ê²°ì •
            if file_size > self.max_file_size_bytes:
                logger.info(f"íŒŒì¼ì´ í½ë‹ˆë‹¤ ({file_size/(1024*1024):.1f}MB > {self.max_file_size_bytes/(1024*1024)}MB). ë¶„í•  ì²˜ë¦¬ ì‹œì‘...")
                return self._transcribe_large_file(audio_path, language)
            else:
                return self._transcribe_single_file(audio_path, language)
                
        except Exception as e:
            logger.error(f"ìŒì„±ì¸ì‹ ì‹¤íŒ¨: {e}")
            raise
    
    def _transcribe_single_file(self, audio_path, language='ko'):
        """ë‹¨ì¼ íŒŒì¼ ìŒì„±ì¸ì‹ ì²˜ë¦¬"""
        with open(audio_path, 'rb') as audio_file:
            response = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language=language,
                response_format="verbose_json"
            )
        
        text = response.text.strip()
        duration = getattr(response, 'duration', 0)
        language_detected = getattr(response, 'language', language)
        
        # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ ì²˜ë¦¬
        segments = self._process_word_timestamps(response, duration)
        
        logger.info(f"ìŒì„±ì¸ì‹ ì™„ë£Œ: {len(text)}ì í…ìŠ¤íŠ¸ ìƒì„±")
        
        return {
            'text': text,
            'language': language_detected,
            'segments': segments,
            'duration': duration,
            'split_count': 1
        }
    
    def _transcribe_large_file(self, audio_path, language='ko'):
        """í° íŒŒì¼ì„ ë¶„í• í•˜ì—¬ ìŒì„±ì¸ì‹ ì²˜ë¦¬"""
        split_files = None
        try:
            # íŒŒì¼ ë¶„í• 
            split_files = self.split_audio_file(audio_path)
            
            if len(split_files) == 1:
                # ë¶„í• ì´ ì‹¤ì œë¡œ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ
                return self._transcribe_single_file(audio_path, language)
            
            # ê° ë¶„í•  íŒŒì¼ ì²˜ë¦¬
            all_texts = []
            all_segments = []
            total_duration = 0
            current_offset = 0
            
            for i, split_file in enumerate(split_files):
                logger.info(f"ë¶„í•  íŒŒì¼ ì²˜ë¦¬ ì¤‘: {i+1}/{len(split_files)}")
                
                result = self._transcribe_single_file(split_file, language)
                all_texts.append(result['text'])
                
                # ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ ì˜¤í”„ì…‹ ì¡°ì •
                for segment in result['segments']:
                    adjusted_segment = segment.copy()
                    adjusted_segment['start'] += current_offset
                    adjusted_segment['end'] += current_offset
                    all_segments.append(adjusted_segment)
                
                current_offset += result['duration']
                total_duration += result['duration']
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            combined_text = ' '.join(all_texts)
            
            logger.info(f"ë¶„í•  ì²˜ë¦¬ ì™„ë£Œ: {len(split_files)}ê°œ íŒŒì¼, ì´ {len(combined_text)}ì")
            
            return {
                'text': combined_text,
                'language': language,
                'segments': all_segments,
                'duration': total_duration,
                'split_count': len(split_files)
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if split_files and len(split_files) > 1:
                self._cleanup_split_files(split_files)
    
    def _process_word_timestamps(self, response, duration):
        """ì‘ë‹µì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        segments = []
        if hasattr(response, 'segments') and response.segments:
            # OpenAI APIì˜ ê¸°ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ìš©
            for segment in response.segments:
                segments.append({
                    'start': segment.get('start', 0),
                    'end': segment.get('end', duration),
                    'text': segment.get('text', '').strip()
                })
        else:
            # ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì²˜ë¦¬
            segments.append({
                'start': 0,
                'end': duration,
                'text': response.text.strip()
            })
        
        return segments
    
    def _cleanup_split_files(self, split_files):
        """ë¶„í• ëœ ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬"""
        try:
            if split_files:
                temp_dir = os.path.dirname(split_files[0])
                if temp_dir and 'vocal_split_' in temp_dir:
                    shutil.rmtree(temp_dir)
                    logger.info("ì„ì‹œ ë¶„í•  íŒŒì¼ë“¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def process_vocal_files(self, input_dir, output_dir):
        """
        ì…ë ¥ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  _vocal.wav íŒŒì¼ì„ ì²˜ë¦¬ (ìë™ ë¶„í•  ì§€ì›)
        
        Args:
            input_dir (str): _vocal.wav íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            output_dir (str): í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬
            
        Returns:
            dict: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path.mkdir(parents=True, exist_ok=True)
        
        # _vocal.wav íŒŒì¼ë“¤ ì°¾ê¸°
        vocal_files = list(input_path.glob("*_vocal.wav"))
        
        if not vocal_files:
            logger.warning(f"{input_dir}ì—ì„œ _vocal.wav íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {'processed': 0, 'failed': 0, 'files': []}
        
        logger.info(f"ì²˜ë¦¬í•  ë³´ì»¬ íŒŒì¼ {len(vocal_files)}ê°œ ë°œê²¬")
        
        results = []
        processed = 0
        failed = 0
        
        for vocal_file in vocal_files:
            try:
                logger.info(f"ì²˜ë¦¬ ì¤‘: {vocal_file.name}")
                
                file_size = vocal_file.stat().st_size
                
                # ìŒì„±ì¸ì‹ ìˆ˜í–‰ (ìë™ ë¶„í•  í¬í•¨)
                transcription = self.transcribe_audio(str(vocal_file))
                
                # ì¶œë ¥ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: ê°€ìš”1_vocal.wav -> ê°€ìš”1_vocal.txt)
                output_filename = vocal_file.stem + '.txt'
                output_file_path = output_path / output_filename
                
                # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
                with open(output_file_path, 'w', encoding='utf-8') as f:
                    f.write(f"íŒŒì¼ëª…: {vocal_file.name}\n")
                    f.write(f"íŒŒì¼ í¬ê¸°: {file_size/(1024*1024):.1f}MB\n")
                    f.write(f"ì–¸ì–´: {transcription['language']}\n")
                    f.write(f"ê¸¸ì´: {transcription['duration']:.2f}ì´ˆ\n")
                    f.write(f"ë¶„í•  ì²˜ë¦¬: {'ì˜ˆ' if transcription['split_count'] > 1 else 'ì•„ë‹ˆì˜¤'} ({transcription['split_count']}ê°œ ì„¸ê·¸ë¨¼íŠ¸)\n")
                    f.write(f"ìƒì„± ì‹œê°„: {self._get_current_time()}\n")
                    f.write(f"ì²˜ë¦¬ ë°©ì‹: OpenAI API (whisper-1)\n")
                    f.write("-" * 50 + "\n\n")
                    f.write(transcription['text'])
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ë³„ ìƒì„¸ ì •ë³´ë„ ì¶”ê°€
                    if transcription['segments']:
                        f.write("\n\n" + "="*50 + "\n")
                        f.write("ì„¸ê·¸ë¨¼íŠ¸ë³„ ìƒì„¸ ì •ë³´:\n")
                        f.write("="*50 + "\n\n")
                        
                        for i, segment in enumerate(transcription['segments'], 1):
                            start_time = segment['start']
                            end_time = segment['end']
                            text = segment['text']
                            f.write(f"[{i}] {start_time:.2f}s - {end_time:.2f}s: {text}\n")
                
                results.append({
                    'input_file': vocal_file.name,
                    'output_file': output_filename,
                    'text_length': len(transcription['text']),
                    'duration': transcription['duration'],
                    'file_size_mb': file_size/(1024*1024),
                    'split_count': transcription['split_count'],
                    'status': 'success'
                })
                
                processed += 1
                logger.info(f"ì™„ë£Œ: {output_filename} ì €ì¥")
                
            except Exception as e:
                logger.error(f"{vocal_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                results.append({
                    'input_file': vocal_file.name,
                    'output_file': None,
                    'error': str(e),
                    'status': 'failed'
                })
                failed += 1
        
        # í†µê³„ ì •ë³´ ë¡œê¹…
        logger.info(f"ì²˜ë¦¬ ì™„ë£Œ - ì„±ê³µ: {processed}ê°œ, ì‹¤íŒ¨: {failed}ê°œ")
        
        return {
            'processed': processed,
            'failed': failed,
            'files': results
        }
    
    def _get_current_time(self):
        """í˜„ì¬ ì‹œê°„ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    try:
        # VoiceToText ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (OpenAI API ì‚¬ìš©, 20MB ê¸°ì¤€ìœ¼ë¡œ ë¶„í• )
        vtt = VoiceToText(max_file_size_mb=20)
        
        # test_result í´ë”ì˜ ë³´ì»¬ íŒŒì¼ë“¤ì„ test_textë¡œ ë³€í™˜
        results = vtt.process_vocal_files(
            input_dir='test_result',
            output_dir='test_text'
        )
        
        print(f"\nì²˜ë¦¬ ê²°ê³¼:")
        print(f"ì„±ê³µ: {results['processed']}ê°œ")
        print(f"ì‹¤íŒ¨: {results['failed']}ê°œ")
        
        if results['files']:
            print("\níŒŒì¼ë³„ ìƒì„¸ ê²°ê³¼:")
            for file_result in results['files']:
                if file_result['status'] == 'success':
                    # ì˜ˆìƒ ë¹„ìš© ê³„ì‚° (1ë¶„ë‹¹ $0.006)
                    cost = (file_result['duration'] / 60) * 0.006
                    split_info = f" (ë¶„í• : {file_result['split_count']}ê°œ)" if file_result['split_count'] > 1 else ""
                    
                    print(f"âœ… {file_result['input_file']} -> {file_result['output_file']}{split_info}")
                    print(f"   ğŸ“„ í…ìŠ¤íŠ¸ ê¸¸ì´: {file_result['text_length']}ì")
                    print(f"   â±ï¸  ì˜¤ë””ì˜¤ ê¸¸ì´: {file_result['duration']:.1f}ì´ˆ")
                    print(f"   ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_result['file_size_mb']:.1f}MB")
                    print(f"   ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${cost:.4f}")
                    print()
                else:
                    print(f"âŒ {file_result['input_file']}: {file_result['error']}")
                    print()
                    
    except ValueError as e:
        print(f"âŒ ì„¤ì • ì˜¤ë¥˜: {e}")
        print("\ní•´ê²° ë°©ë²•:")
        print("1. .env íŒŒì¼ì— OPENAI_API_KEY=your_api_key_here ì¶”ê°€")
        print("2. ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ export OPENAI_API_KEY=your_api_key_here") 